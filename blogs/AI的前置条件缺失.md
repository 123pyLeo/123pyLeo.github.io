---
layout: page
permalink: /blogs/AI的前置条件缺失/index.html
title: AI的前置条件缺失
---

在这一段时间与各种 ai 大模型的高强度交互中，我逐渐意识到一个比幻觉更隐蔽的问题：ai 给出的回答往往在逻辑上是成立的，但它默认省略了结论成立所依赖的前置条件。  

这类回答并非明显错误，甚至常常看起来非常有道理。正因如此，我会下意识地将其视为可直接执行的建议，并据此修改代码、调整结构或重训模型。  

然而，当这些隐含条件在我的实际场景中并不成立时，问题并不会立刻显现，而是以副作用、奇奇怪怪的性能指标的形式逐步暴露出来。  

或许正确但不完整的结论，比错误更危险。相比于明显的幻觉，这种条件缺失的正确结论更具迷惑性。  

幻觉往往在实践中较容易被识别和否定，而缺乏前置条件说明的结论，则会在执行后才显露问题，此时已经付出了时间、算力和结构修改的成本。  

当我带着这些问题再次向 ai 追问时，模型通常能够给出补充解释，逐步补齐之前未提及的假设或适用范围。  

但在这一阶段，这些解释更像是一种事后修补，而非事前的风险提示。  

对于需要实际落地执行的研究或工程任务而言，这种亡羊补牢式的纠偏往往已经过晚。  

当前的大模型更倾向于提供简洁、连贯、像专家回答的结论，而不是像研究报告一样系统性地枚举假设条件、边界情况和失败模式。  

当 ai 的回答只用于启发思路或生成假设时，其不完整性是可接受的；  

但当回答被直接视为可执行方案时，前置条件的缺失就成为不可忽视的问题。  

因此，在后续的使用中，我觉得可以从这个结论是否合理转向这个结论在什么条件下才成立。  

只有当这些条件具备时，结论才值得被真正采纳并付诸实践。